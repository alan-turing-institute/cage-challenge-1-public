Failure # 1 (occurred at 2022-01-24_12-55-48)
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/ray/tune/trial_runner.py", line 924, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "/usr/local/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py", line 787, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/usr/local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/ray/worker.py", line 1715, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=6894, ip=127.0.0.1)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py", line 102, in __init__
    Trainer.__init__(self, config, env, logger_creator,
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 661, in __init__
    super().__init__(config, logger_creator, remote_checkpoint_dir,
  File "/usr/local/lib/python3.9/site-packages/ray/tune/trainable.py", line 121, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py", line 113, in setup
    super().setup(config)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 764, in setup
    self._init(self.config, self.env_creator)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py", line 136, in _init
    self.workers = self._make_workers(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/agents/trainer.py", line 1727, in _make_workers
    return WorkerSet(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 110, in __init__
    self._local_worker = self._make_worker(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 449, in _make_worker
    worker = cls(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 587, in __init__
    self._build_policy_map(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1551, in _build_policy_map
    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py", line 139, in create_policy
    class_(observation_space, action_space, merged_config)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 136, in __init__
    super(TracedEagerPolicy, self).__init__(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 414, in __init__
    self._initialize_loss_from_dummy_batch(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/policy.py", line 799, in _initialize_loss_from_dummy_batch
    self.compute_actions_from_input_dict(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 118, in _func
    return obj(self_, *args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 162, in compute_actions_from_input_dict
    return super(TracedEagerPolicy, self).\
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 464, in compute_actions_from_input_dict
    ret = self._compute_actions_helper(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/utils/threading.py", line 21, in wrapper
    return func(self, *a, **k)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py", line 783, in _compute_actions_helper
    dist_inputs, state_out = self.model(
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/models/modelv2.py", line 244, in __call__
    res = self.forward(restored, state or [], seq_lens)
  File "/Users/chicks/OneDrive - The Alan Turing Institute/Documents/Development/CybORG/cage-challenge-1/agents/rllib_alt/train_rllib_alt.py", line 87, in forward
    return self.model.forward(input_dict, state, seq_lens)
  File "/usr/local/lib/python3.9/site-packages/ray/rllib/models/tf/recurrent_net.py", line 65, in forward
    assert seq_lens is not None
AssertionError

