- config: general-flags
  flags: 
    name:
      description: Name of person running this
      default: ATI
      type: string
    team: 
      description: Team name...
      default: Mindrake
      type: string
    name-of-agent:
      description: Name of the agent
      default: a2c
      choice:
        - rnd
        - a2c-rnd
        - a2c
        - ppo
        - rllib-alt
      type: string
    scenario:
      description: Selected scenario
      default: Scenario1b
      choice:
        - Scenario1
        - Scenario1b
      type: string
    seed: 
      description: Random seed
      default: 0
      type: int
- config: train-test-flags
  flags: 
    continuous-action-space:
      description: continuous action space; else discrete
      arg-switch: no
    batch-size: 
      description: Minibatch size
      default: 128
      type: int
    update-step:
      description: Update step
      default: 50
      type: int
    episode-length:
      description: Episode length
      default: 100
      type: int
    training-length:
      description: Training length
      default: 4000
      type: int
    max-eps:
      description: Maximum episodes (MAX_EPS)
      default: 10
      type: int
    red-agent:
      description: Selected Red agent
      default: 'B_lineAgent'
      choices:
        - 'B_lineAgent'
        - 'RedMeanderAgent'
        - 'SleepAgent'

- config: agent-flags
  flags:
    gamma: 
      description: discount factor
      default: 0.9
      type: float
    checkpoint-path:
      description: Relative path to checkpoint/saved model

- model: a2c
  description: A2C Agent
  sourcecode:
    select: 
      - "agents/a2c/a2c/a2c_agent.py"
      - "agents/a2c/a2c/a2c.py"
      - "agents/a2c/a2c/distributions.py"
      - "agents/a2c/a2c/rnd/rnd.py"
      - "agents/a2c/env_utils.py"
      - "agents/a2c/a2c/rollout.py"
      - "agents_list.py"
      - "config.py"
      - "evaluation.py"
      - "train_a2c.py"
    exclude: 
      dir: tutorial_code
    dest: results/saved_models/a2c
  params: 
    default-gamma: 0.9
    default-epsilon: 0.001
    default-learning-rate: 0.0007
  operations: 
    train: 
      description: Challenge evaluation script using A2C agent
      main: train_a2c
      flags: 
        $include: 
          - general-flags
          - train-test-flags
        gamma: '{{ default-gamma }}'
        checkpoint-path: "results/saved_models/a2c"
        epsilon: 
          description: See MF A2C agent docs...
          default: '{{ default-epsilon }}'
          type: float
        learning-rate: 
          description: See MF A2C agent docs...
          default: '{{ default-learning-rate }}'
          type: float
        priority: 
          description: See MF A2C agent docs...
          arg-switch: no
        exploring-steps:
          description: See MF A2C agent docs...
          default: 100
          type: int
        rnd: 
          description: See MF A2C agent docs...
          arg-switch: no
        attention: 
          description: See MF A2C agent docs...
          arg-switch: no
        pre-obs-norm:
          description: See MF A2C agent docs...
          default: 10
          type: int
        action-space:
          description: See MF A2C agent docs...
          default: 54
          type: int
        obs-space:
          description: See MF A2C agent docs...
          default: 52
          type: int
        val-loss-coef:
          description: See MF A2C agent docs...
          default: 0.5
          type: float
        entropy-coef:
          description: See MF A2C agent docs...
          default: 0.0
          type: float
        max-grad-norm:
          description: See MF A2C agent docs...
          default: 0.5
          type: float
        alpha:
          description: See MF A2C agent docs...
          default: 0.99
          type: float
        update_prop:
          description: See MF A2C agent docs...
          default: 0.25
          type: float
        processes:
          description: See MF A2C agent docs...
          default: 4
          type: int
      output-scalars: 
        step: '^(\step)'
        agent: 'AGENT:  (\value)'
        ep_loss_av: 'EP_LOSS_AV:   (\value)'
        ep_reward: 'EP_REWARD:    (\value)'
        rewards: 'REWARD MIN/MAX/MEAN/SD: (?P<reward_min>\value) (?P<reward_max>\value) (?P<reward_mean>\value) (?P<reward_std>\value)'
        action: 'ACTION: ([\w\s]+)   $'
    eval: 
      description: Challenge evaluation script using A2C agent
      main: evaluation
      flags: 
        $include: 
          - general-flags
          - train-test-flags
        gamma: '{{ default-gamma }}'
        checkpoint-path: "results/saved_models/a2c/actor_critic.pt"
        batch-size: 100
        epsilon: 
          description: See MF A2C agent docs...
          default: '{{ default-epsilon }}'
          type: float
        learning-rate: 
          description: See MF A2C agent docs...
          default: '{{ default-learning-rate }}'
          type: float
        priority: 
          description: See MF A2C agent docs...
          arg-switch: no
        exploring-steps:
          description: See MF A2C agent docs...
          default: 100
          type: int
        rnd: 
          description: See MF A2C agent docs...
          arg-switch: no
        attention: 
          description: See MF A2C agent docs...
          arg-switch: no
        pre-obs-norm:
          description: See MF A2C agent docs...
          default: 10
          type: int
        action-space:
          description: See MF A2C agent docs...
          default: 54
          type: int
        obs-space:
          description: See MF A2C agent docs...
          default: 52
          type: int
        val-loss-coef:
          description: See MF A2C agent docs...
          default: 0.5
          type: float
        entropy-coef:
          description: See MF A2C agent docs...
          default: 0.0
          type: float
        max-grad-norm:
          description: See MF A2C agent docs...
          default: 0.5
          type: float
        alpha:
          description: See MF A2C agent docs...
          default: 0.99
          type: float
        update_prop:
          description: See MF A2C agent docs...
          default: 0.25
          type: float
        processes:
          description: See MF A2C agent docs...
          default: 4
          type: int
      output-scalars: 
        step: '^(\step)'
        agent: 'AGENT:  (\value)'
        ep_loss_av: 'EP_LOSS_AV:   (\value)'
        ep_reward: 'EP_REWARD:    (\value)'
        rewards: 'REWARD MIN/MAX/MEAN/SD: (?P<reward_min>\value) (?P<reward_max>\value) (?P<reward_mean>\value) (?P<reward_std>\value)'
        action: 'ACTION: ([\w\s]+)   $'
- model: ppo
  description: PPO Agent
  sourcecode:
    select: 
      - "agents/ppo/PPO.py"
      - "agents_list.py"
      - "config.py"
      - "evaluation.py"
    exclude: 
      dir: tutorial_code
    dest: results/saved_models/a2c
  params: 
    default-gamma: 0.99
  operations: 
    eval: 
      description: Evaluation script for PPO agent
      main: evaluation
      flags: 
        $include: 
          - general-flags
          - train-test-flags
        gamma: '{{ default-gamma }}'
        checkpoint-path: "results/saved_models/ppo/ppo_default.pt"
        action-std:
          description: starting std for action distribution (Multivariate Normal)
          default: 0.6
          type: float
        action-std-decay-rate:
          description: linearly decay action_std (action_std = action_std - action_std_decay_rate)
          default: 0.05
          type: float
        min-action-std:
          description: minimum action_std (stop decay after action_std <= min_action_std)
          default: 0.1  
          type: float
        action-std-decay-freq:
          description: action_std decay frequency (in num timesteps)
          default: 250000
          type: int
        k-epochs:
          description: update policy for K epochs in one PPO update
          default: 80
          type: int
        eps-clip:
          description: clip parameter for PPO
          default: 0.2
          type: float
        lr-actor:
          description: learning rate for actor network
          default: 0.0003
          type: float
        lr-critic:
          description: learning rate for critic network
          default: 0.001
          type: float
      output-scalars: 
        - run_time: 'Total run time: \n(\value)'
- model: rllib-alt
  description: RLLib Baseline Agent
  sourcecode:
    select: 
      - "train_rllib_alt.py"
      - "agents_list.py"
      - "config.py"
      - "evaluation.py"
    exclude: 
      dir: tutorial_code
    dest: agents/rllib_alt/log_dir
  params: 
    default-epsilon: 0.1
    default-learning-rate: 0.0001
  operations:
    train: 
      description: Train agent
      main: train_rllib_alt
      flags: 
        $include: 
          - general-flags
        checkpoint-path: "agents/rllib_alt/log_dir"
        epsilon: 
          description: Exploration probability
          default: 0.1
          type: float
        learning-rate: 
          description: Step-size parameter
          default: 0.0001
          type: float
    eval: 
      description: Evaluation script for agent
      main: evaluation
      flags: 
        $include: 
          - general-flags
          - train-test-flags
        gamma: '{{ default-gamma }}'
        checkpoint-path: "agents/rllib_alt/log_dir/IMPALA_2022-01-22_14-26-59/IMPALA_CybORGAgent_5be72_00000_0_lr=0.0001_2022-01-22_14-26-59/checkpoint_000612/checkpoint-612"